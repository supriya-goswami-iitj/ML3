# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVx7SOnNNhYbRPOpoqhRFL-1GCBSelVD
"""

#importing the required libraries

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense
import cv2
import os
from os import listdir
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import array_to_img, img_to_array, load_img


# get the path/directory
from google.colab import drive
drive.mount('/content/drive')

#loading data, data collection

testpath='/content/drive/MyDrive/charts/charts/test'
trainpath='/content/drive/MyDrive/charts/charts/train_val'
dfTrain = pd.DataFrame(columns=['Index', 'Image','Type','MeanPx', 'STDPx'])
dfTest = pd.DataFrame(columns=['Index', 'Image','Type','MeanPx', 'STDPx'])

# Print DataFrame Snapshot

df = pd.read_csv("/content/drive/MyDrive/charts/charts/train_val.csv")
print(df.head(20))
types = ["vbar_categorical", "hbar_categorical", "line", "pie", "dot_line"]

img_width, img_height = 0,0
xgen = ()

train_data_dir = trainpath
validation_data_dir = testpath
nb_train_samples = 800
nb_validation_samples = 200
epochs = 50
batch_size = 16


datagen = tf.keras.preprocessing.image.ImageDataGenerator(
       featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

for ind in df.image_index:      
       # Load an color image in grayscale
      
      img = cv2.imread(trainpath+"/"+str(ind)+".png", cv2.IMREAD_GRAYSCALE)
      img2 = cv2.imread(testpath+"/"+str(ind)+".png", cv2.IMREAD_GRAYSCALE)
      img //= 255
      np.save('pic.npy', img)
      np.save('pic2.npy', img)
      dfTrain.at[ind, 'Index'] = ind
      #dfTrain.at[ind, 'Image'] = np.load('pic.npy')
      dfTrain.at[ind, 'MeanPx'] = np.average(img)
      dfTrain.at[ind, 'STDPx'] = np.std(img)
      dfTrain.at[ind, 'Type'] = types.index(df['type'][ind])

      # dimensions of our images.
      img_width, img_height = img.shape

      

      imgpil = load_img(trainpath+"/"+str(ind)+".png")  # this is a PIL image
      xgen = img_to_array(img)  # this is a Numpy array with shape (3, 128, 128)
      xgen = xgen.reshape((1,) + xgen.shape)  # this is a Numpy array with shape (1, 3, 128, 128)

      i = 0
      for batch in datagen.flow(xgen, batch_size=1,save_to_dir=trainpath, save_prefix=str(ind), save_format='png'):
      # The batch will be an array of integers of data generated from image array
       #print(batch)
       i += 1
       if i > 20:
        break  # otherwise the generator would loop indefinitely

       #print(xgen)

      if not img2 is None:
       img2 //= 255
       dfTest.at[ind, 'Index'] = ind
       #dfTest.at[ind, 'Image'] = np.load('pic2.npy')
       dfTest.at[ind, 'MeanPx'] = np.average(img2)
       dfTest.at[ind, 'STDPx'] = np.std(img2)
       dfTest.at[ind, 'Type'] = types.index(df['type'][ind])


# print the training and test data   
print(dfTrain)
print(dfTest)

#Data Visulaisation

plt.plot(dfTrain.sample(n=50,replace=True), dfTest)

plt.title( 'dfTrain vs dfTest' )
	
plt.xlabel( 'X' )
	
plt.ylabel( 'Y' )
	
plt.show()

X_train, X_test = train_test_split(dfTrain,train_size=0.8,random_state=25)
Y_train, Y_test = train_test_split(dfTest,test_size=0.2,random_state=25)


#checking the shape of data arrays
print("shape of training and test data X and Y in order")
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

#defining model
model=Sequential()

#adding convolution layer
model.add(Conv2D(32,(3,3),activation='relu',input_shape=(128,128,3)))

#adding pooling layer
model.add(MaxPool2D(2,2))

#adding fully connected layer
model.add(Flatten())
model.add(Dense(64,activation='relu'))

#adding output layer
model.add(Dense(32,activation='softmax'))

#compiling the model
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])


#reshaping and resizing the model so that it can fit as per the input pattern if required via error thrown

#X_train = X_train.to_numpy().reshape(*X_train.shape, 1)
#Y_train = Y_train.to_numpy().reshape(*Y_train.shape, 1)

#X_train = np.zeros((X_train.shape[1],1,1,5))
#Y_train = np.zeros((Y_train.shape[1],1,1,5))

#plotting based on various parameters

#plt.plot(X_train, Y_train)

#converting to tensor flow

X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)
Y_train_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)
print(X_train_tf.shape)
print(Y_train_tf.shape)
#tf.reshape(X_train_tf, (40,5,20))
#tf.reshape(Y_train_tf, (5,5))
X_test_tf = tf.convert_to_tensor(X_test.sample(n=10,replace=True), dtype=tf.float32)
Y_test_tf = tf.convert_to_tensor(Y_test, dtype=tf.float32)
#tf.expand_dims(X_train_tf,axis=1)
#fitting the model

model.summary()
#tf.reduce_min(Y_train_tf)

history = model.fit(X_train_tf[:40],Y_train_tf,epochs=500,validation_data = (X_test, Y_test))

#metrics print

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

print(acc)

print(history.history)
training_loss = history.history['loss']
test_loss = history.history['loss']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Training Loss', 'Test Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show();




model.evaluate(X_test_tf,Y_test_tf)

model.predict(X_test_tf)

#Pretrained Models (Data Augmentation)

datagen = tf.keras.preprocessing.image.ImageDataGenerator(
       featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(X_train_tf) #Input Rank needs to be met, reshape is required


if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit_generator(train_generator,steps_per_epoch=nb_train_samples, epochs=epochs, validation_data=validation_generator,validation_steps=nb_validation_samples)

model.save_weights('first_try.h5')