# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G5reUSwzoFPLnlSgHpMQaRBQLtiLKpTp
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing Libraries

import numpy as np
import torch
import tensorflow as tf
from tensorflow import keras
from keras import backend as K
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline
import cv2
import os
from os import listdir
from sklearn.model_selection import train_test_split
from PIL import Image
from google.colab.patches import cv2_imshow
 
# get the path/directory for training data set
from google.colab import drive
drive.mount('/content/drive')
path='/content/drive/MyDrive/GurNum/train'

dfTrain = pd.DataFrame(columns=['Label', 'Pixel', 'MeanPx', 'STDPx'])
dfTest = pd.DataFrame(columns=['Label', 'Pixel', 'MeanPx', 'STDPx'])
for dir in os.listdir(path):
  dirpath = path+('/')+(str(dir))
  for image in os.listdir(dirpath):  
     # Load color image in grayscale
     n_classes = 10
     img = cv2.imread(dirpath+"/"+image, cv2.IMREAD_GRAYSCALE) # The image pixels have range [0, 255]
     img //= 255  # Now the pixels have range [0, 1]
     
     # Create the data frame
     dfTrain.loc[dir, 'Label'] = dir
     dfTrain.loc[dir, 'Pixel'] = img
     dfTrain.loc[dir, 'MeanPx'] = np.average(img)
     dfTrain.loc[dir, 'STDPx'] = np.std(img)
     
     #The grayscale images are combination of white and black pixels
     #Printing the real images below using avg of BGR pixel values and conversion to gray scale
          
     img_list = img.tolist() #  a list of lists of pixels

     result = ""
     for row in img_list:
      row_str = [str(p) for p in row]
      result += "[" + ", ".join(row_str) + "],\n"
     img = cv2.imread(dirpath+"/"+image)
     (row, col) = img.shape[0:2]
     # Take the average of pixel values of the BGR Channels
     # to convert the colored image to grayscale image
     for i in range(row):
      for j in range(col):
        # Find the average of the BGR pixel values
        img[i, j] = sum(img[i, j]) * 0.33
  
     cv2_imshow(img)  
 
# get the path/directory for test data set
path='/content/drive/MyDrive/GurNum/val'


for dir in os.listdir(path):
  dirpath = path+('/')+(str(dir))
  for image in os.listdir(dirpath):  
     # Load an color image in grayscale
     im = Image.open(dirpath+"/"+image)
     orig_pixel_map = im.load()
     for x in range(10):
      for y in range(10):
       pixel = orig_pixel_map[x, y]

# Using CV2 for the purpose of creating a data frame.

  img = cv2.imread(dirpath+"/"+image, cv2.IMREAD_GRAYSCALE) # The image pixels have range [0, 255]
  img //= 255  # Now the pixels have range [0, 1]

  #Using image pixel mean and standard deviation as the deciding factor for classification
     
  dfTest.loc[dir, 'Label'] = dir
  dfTest.loc[dir, 'Pixel'] = img
  dfTest.loc[dir, 'MeanPx'] = np.average(img)
  dfTest.loc[dir, 'STDPx'] = np.std(img)
dfTrain.sort_values(by=['Label'])
dfTest.sort_values(by=['Label'])

# Print the data frames
print(dfTrain.sort_values(by=['Label']))
print(dfTest.sort_values(by=['Label']))

#Split the validation and training data
X_train, X_test = train_test_split(dfTrain[['MeanPx','STDPx']],train_size=0.5,random_state=25)
y_train, y_test = train_test_split(dfTest.MeanPx,test_size=0.5,random_state=25)

print("X_train")
print(X_train)

print("X_test")
print(X_test)

print("y_train")
print(y_train)

print("y_test")
print(y_test)

# data normalization/scaling

X_train_scaled = X_train.copy()
X_train_scaled[['MeanPx']]= X_train_scaled[['MeanPx']] / 100
X_train_scaled[['STDPx']]= X_train_scaled[['STDPx']] / 100

print("X_train_scaled")
print(X_train_scaled)

X_test_scaled = X_test.copy()
X_test_scaled[['MeanPx']] = X_test_scaled[['MeanPx']] / 100
X_test_scaled[['STDPx']]= X_test_scaled[['STDPx']] / 100

print("X_test_scaled")
print(X_test_scaled)


#data visualization, plotting against different parameters to see dependency

meanpx = dfTrain['MeanPx'].head(12)
stdpx = dfTest['STDPx'].head(12)
 

plt.plot(X_train, y_train)

plt.title( 'X_train vs y_train' )
	
plt.xlabel( 'X' )
	
plt.ylabel( 'Y' )
	
plt.show()
plt.scatter(X_train_scaled, X_test_scaled, color = 'green')
plt.scatter(y_train, y_test, color = 'brown')
plt.title( 'X vs Y' )
	
plt.xlabel( 'X' )
	
plt.ylabel( 'Y' )
plt.show()
plt.plot(y_train, y_test, color = 'pink')

plt.title( 'Y vs X' )
	
plt.xlabel( 'X' )
	
plt.ylabel( 'Y' )
plt.show()



#Keras for comparison

model = keras.Sequential([
keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')
])
def accuracy(y_true, y_pred):
    return K.mean(100-(abs(y_pred/y_true - 1) * 100))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=[accuracy])


#Convert to tensor compatible format

X_train_scaled_tf = tf.convert_to_tensor(X_train_scaled.to_numpy(), dtype=tf.float32)


print("X_train_scaled_tf")
print(X_train_scaled_tf)

y_train_tf = tf.convert_to_tensor(y_train.to_numpy(), dtype=tf.float32)

print("y_train_tf")
print(y_train_tf)

history=model.fit(X_train_scaled_tf, y_train_tf, epochs=5000)
print(history.history)


X_test_scaled_tf = tf.convert_to_tensor(X_test_scaled.to_numpy(), dtype=tf.float32) 

print("X_test_scaled_tf")
print(X_test_scaled_tf)
y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)

print("y_test_tf")
print(y_test_tf)

# Get training and test loss histories
training_loss = history.history['loss']

history=model.fit(X_test_scaled_tf, y_test_tf, epochs=5000)
print(history.history)

test_loss = history.history['loss']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b-')
plt.legend(['Training Loss', 'Test Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show();




model.evaluate(X_test_scaled_tf,y_test_tf)

model.predict(X_test_scaled_tf)


coef, intercept = model.get_weights()
print("coef")
print(coef)
print("intercept")
print(intercept)

# Custom Model

def sigmoid(x):
        import math
        return 1 / (1 + math.exp(-x))


def prediction_function(MeanPx, STDPx):
    weighted_sum = coef[0]*MeanPx + coef[1]*STDPx + intercept
    return sigmoid(weighted_sum)



def sigmoid_numpy(X):
   return 1/(1+np.exp(-X))


def cross_entropy_log_loss(y_true, y_predicted):
    epsilon = 1e-15
    y_predicted_new = [max(i,epsilon) for i in y_predicted]
    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
    y_predicted_new = np.array(y_predicted_new)
    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))


class myNN:
    def __init__(self):
        self.w1 = 1 
        self.w2 = 1
        self.bias = 0
        
    def fit(self, X, y, epochs, loss_thresold):
        self.w1, self.w2, self.bias = self.gradient_descent(X['MeanPx'],X['STDPx'],y, epochs, loss_thresold)
        print(f"Final weights and bias: w1: {self.w1}, w2: {self.w2}, bias: {self.bias}")
        
    def predict(self, X_test):
        weighted_sum = self.w1*X_test['MeanPx'] + self.w2*X_test['STDPx'] + self.bias
        print(sigmoid_numpy(weighted_sum))
        return sigmoid_numpy(weighted_sum)

    def gradient_descent(self, MeanPx,STDPx, y_true, epochs, loss_thresold):
        w1 = w2 = 1
        bias = 0
        rate = 0.5
        n = len(MeanPx)
        for i in range(epochs):
            weighted_sum = w1 * MeanPx + w2 * STDPx + bias
            y_predicted = sigmoid_numpy(weighted_sum)
            loss = cross_entropy_log_loss(y_true, y_predicted)
            
            A=(y_predicted-y_true)
            
            A[np.isnan(A)] = 0
            A=A[:-2]
            w1d = (1/n)*np.dot(np.transpose(MeanPx),A) 
            w2d = (1/n)*np.dot(np.transpose(STDPx),A) 

            bias_d = np.mean(y_predicted-y_true)
            w1 = w1 - rate * w1d
            w2 = w2 - rate * w2d
            bias = bias - rate * bias_d
            
            if i%50==0:
                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')
            
            if loss<=loss_thresold:
                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')
                break

            
        return w1, w2, bias

customModel = myNN()
X_train_scaled = X_train_scaled.astype(float)
y_train = y_train.astype(float)

customModel.fit(X_train_scaled, y_train, epochs=8000, loss_thresold=0.4631)
print("X_test_scaled")
print(X_test_scaled)
print("y_train")
print(y_train)



X_test_scaled = X_test_scaled.astype(float)
print("custom model predict")
customModel.predict(X_test_scaled)
print("model predict")
model.predict(X_test_scaled)